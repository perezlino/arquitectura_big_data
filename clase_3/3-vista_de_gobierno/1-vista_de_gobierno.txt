=================
VISTA DE GOBIERNO
=================

Ahora esto que hemos definido, ya nos da estrategias para empezar a capturar datos, pero todavía no estamos gobernando 
los procesos, porque, lo único que hemos hecho es hacer el CTRL + C y el CTRL + V de los datos. Pero ¿qué pasa con las 
reglas de calidad? por ejemplo, porque la data va a venir con datos sucios y eso hay que arreglarlo. Digamos, tenemos la 
tabla ‘Persona’ y tenemos un proceso que implementa las reglas de calidad (reglas de quality), por ejemplo, tienen que 
tener edades mayores a cero, y como resultante obtendremos una tabla ‘Persona’ donde colocaremos los registros que pasan 
las reglas de calidad, y aquellos que no cumplan, van a ir una tabla ‘Personas reyectadas’. Si tenemos 100.000 registros 
en la tabla ‘Persona’ original y en la tabla ‘Persona’ después de haber aplicado la regla de calidad aterrizan 9000 
registros y en la tabla ‘Personas reyectadas’ aterrizan 1000 registros, tendríamos que la fuente de datos que estamos 
ingestando cumplen las reglas de calidad en un 90% y hay un 10% que algo raro está pasando, entonces, el equipo de calidad 
se conectará a la tabla ‘Personas reyectadas’ y empezará a revisar los registros para ver qué es lo que ha pasado. Tenemos 
que tener entonces una capa en donde se empiece a aplicar todo esto, para poder gobernar los datos. 

-----------------------------------------------------------------------------------------------------------------------------

DATA LAKE
---------

Ahora, ¿cómo se hace esto? esto se hace con un un marco de trabajo conocido como DATA LAKE, el cual como mínimo está compuesto 
por cuatro capas, cada una de estas capas tiene un objetivo muy específico, voy a dibujarlo desde cero para poder entenderlo. 
Nosotros hemos dicho que lo primero que tenemos que hacer es tener la estrategia de captura de datos y dijimos que eso se 
escribe en el sistema de Big data, en el sistema de almacenamiento. Pero específicamente se escribe dentro de una capa conocida 
como LANDING TMP. Es una capa de aterrizaje, si son archivos estructurados van a vivir en tablas, si son archivos 
semiestructurados los colocaremos en un directorio, si son archivos no estructurados como imágenes también irán en un 
directorio. Así es como la estrategia de captura de datos ingesta los datos en una capa llamada LANDING TEMPORAL. Ahora, el 
objetivo de la capa de LANDING TEMPORAL es capturar la data del día de hoy. Hay una capa llamada LANDING que tiene el objetivo 
de ir acumulando los datos, por ejemplo, en esta capa tenemos una tabla que va a estar particionada por la fecha en la que 
estamos ingestando los datos y aquí tendremos el archivo de anteayer, está el archivo de ayer y el archivo de hoy. Luego, al 
día siguiente se borrará  el archivo que se encuentra en la capa LANDING_TMP y se ingresará este nuevo archivo a esta capa y lo 
moveremos a la capa LANDING a la tabla en la partición correspondiente y de esa manera vamos a ir acumulando los datos. Eso se 
hace en la zona de LANDING. 


¿Qué es lo que pasa con los datos semiestructurados y no estructurados? 
-----------------------------------------------------------------------

Vamos a tener un directorio en la capa LANDING, digamos que estamos capturando un archivo JSON, dentro de este directorio habrá 
un subdirectorio que dirá: “ … esta es la fecha del día de anteayer, luego esta es el archivo de la fecha del día de ayer y el 
día de hoy creamos un nuevo directorio y guardamos el archivo así y de esa manera vamos capturando los datos … “. Es decir, se 
crearon dentro del directorio, distintos subdirectorios para cada fecha. Lo mismo con los datos no estructurados, un directorio 
con subdirectorios y cada subdirectorio va a tener las imágenes, vídeos o lo que se esté capturando ese día. Por eso esta capa 
se conoce como LANDING TEMPORAL es solamente para escribir los datos desde las fuentes de datos, eso que pusimos en la 
estrategia de ingesta de los datos, eso escribe aquí, pero luego tenemos que moverlo a las particiones o a los subdirectorios. 
Adicionalmente a eso, HAY QUE BINARIZAR LOS DATOS. 


¿Cuál es el problema con lo que hemos definido hasta el momento? 
----------------------------------------------------------------

Recordemos que generalmente los archivos de datos que nos dejan las fuentes de datos son archivos de texto plano que están 
separados por comas o son archivos JSON o son archivos de imágenes. Hay que tratar de binarizar esos datos, por ejemplo, si son 
datos estructurados eso lo vamos a binarizar en un formato conocido como AVRO, los datos semiestructurados también tienen 
soporte para AVRO, los datos no estructurados va a depender la binarización, ahí por ejemplo, vamos a tener que tensorizar. Lo 
importante es que dentro de la capa LANDING ya hay que escoger alguna estrategia de binarización de los datos, porque, los 
entornos de Big data pueden procesar archivos de texto plano, pero, son demasiado lentos. Existen archivos binarizados que 
permiten hacer el procesamiento hasta 10 veces más rápido que si estuviéramos procesando en texto plano. Así que al momento de 
mover el archivo ingestado a la partición de la tabla correspondiente no solamente lo vamos a mover, sino, que también lo vamos 
a binarizar en algún formato de rápido procesamiento. 

Una vez que esté en la zona de LANDING hay que limpiar esos datos, porque, pueden venir con errores. Eso se hace en una capa 
conocida como UNIVERSAL. Implementaremos aquí un proceso que se conecte a la data que se está ingestando el día de hoy y le 
aplique las reglas de calidad y eso como resultante nos entregará una tabla limpia, si estamos ingresando la tabla ‘Persona’ 
en UNIVERSAL tendremos también la tabla ‘Persona’ pero con registros limpios. 

Lo mismo va a pasar con los datos de semi estructurados y no estructurados, pero, acá hay un nivel de complejidad que vamos a 
explicar en unos momentos luego de entender un poquito mejor esto. Lo importante es que en UNIVERSAL ya vive la data de manera 
limpia y lista para ser consultada por las soluciones, eso se hace en la zona de SMART. La zona de SMART se conecta a los datos 
que ya están limpios y ya hace su proceso de ETL, su proceso de reporting, su red neuronal o lo que quiera hacer. Ahora ¿qué es 
lo que pasa? antiguamente se hacía lo siguiente: ya sabemos que en UNIVERSAL vamos a tener los datos limpios, ahora podríamos 
construir un proceso que se conecta a esas tablas de datos ya limpias y las empieza a procesar y por ejemplo aquí salga el 
reporte o salga el modelo de red neuronal o lo que estemos implementando. Pero ¿qué es lo que pasa? como yo les dije, las 
reglas de calidad no necesariamente son las mismas para todas las soluciones que vamos a implementar, por ejemplo, a un proceso 
de negocio si le importa que la edad sea mayor a cero y otro proceso le da exactamente igual, porque, se enfoca en procesar las 
estaturas de las personas. Así que, aquí en la capa UNIVERSAL van reglas de calidad genéricas, por ejemplo, no pueden haber 
registros con todos sus valores nulos, eso se tiene que cumplir para cualquier tabla y en la capa SMART la propia solución ya 
define sus propias reglas de calidad particulares, por ejemplo, en esta solución de la capa SMART la persona debe tener edades 
mayores a cero. Digamos que otra solución que hace otra cosa con la tabla limpia de la capa UNIVERSAL, la edad le da 
exactamente igual, puede tener edad mayor a cero, no le interesa, pero las estaturas tienen que ser mayores a cero y tiene que 
tener registrado un correo electrónico en el campo de correo porque le quiere enviar un correo. Entonces, cada una de las 
soluciones define sus propias reglas de calidad. El output de una tabla con reglas de calidad particulares (en la capa SMART) 
va una pequeña base de datos conocida como DATA MESH. El DATA MESH lo que tiene son las resultantes de reglas de calidad 
particulares a cada solución y ahora sí construimos la solución, quizá quiere hacer un proceso de reportería, quizá quiere 
hacer un modelo de red neuronal. En esencia, esto es lo que hace un DATA LAKE. Estas 2 capas, las capas LANDING_TMP y LANDING,
HACEN UN PROCESO DE ETL, CAPTURA DE DATOS Y BINARIZACIÓN. La capa UNIVERSAL HACE UN PROCESO CONOCIDO COMO MODELAMIENTO DE LOS 
DATOS, le aplica reglas de calidad genéricas y tiene la tabla lista para que las soluciones las utilicen. y esta es la capa de 
soluciones (SMART), EN DONDE TU YA TE PONES A IMPLEMENTAR TODO LO QUE QUIERAS CODIFICAR, DESDE UNA REPORTERÍA HASTA UN MODELO 
DE RED NEURONAL. Dentro de esa capa de soluciones, cada solución define su propio DATA MESH, QUE EN ESENCIA SON REGLAS 
PARTICULARES, QUE CADA SOLUCIÓN NECESITA, QUE NO PUEDE SER REALIZADA EN LA CAPA UNIVERSAL, porque, se contradecirían entre sí. 
Así que cada solución que defina sus propias reglas de calidad, qué es lo que entiende por una tabla ‘Persona’ que esté lista 
para ser procesada. Esas resultantes van en una pequeña base de datos llamada DATA MESH, el cual, ya es el punto de partida 
para poder hacer la solución que tú necesitas. 

Estas capas que hemos definido LANDING_TMP, LANDING, UNIVERSAL y SMART conforman lo que es un DATA LAKE en Big data y ya nos 
ayuda a tener la estrategia de procesamiento, capturamos guardamos de manera permanente, binarizamos, limpiamos, limpiamos con 
reglas particulares y hacemos la solución. CUALQUIER PROCESO QUE IMPLEMENTEMOS TIENE QUE SEGUIR ESTE FLUJO, no es que en la 
zona de soluciones hago todo directamente, está bien distribuido cada parte del procesamiento y de esa manera podemos gobernar 
los flujos que haya dentro del entorno de Big data. 

-----------------------------------------------------------------------------------------------------------------------------

DATOS ESTRUCTURADOS, SEMI-ESTRUCTURADOS Y NO ESTRUCTURADOS
----------------------------------------------------------

Sin embargo, hay que ser más detallados, porque, en la vida real cuando los desarrolladores trabajen, te van a entender 
conceptualmente el flujo que vimos en la diapositiva anterior, pero ¿qué pasa si llega un archivo de vídeo? ¿qué es lo que va 
a hacer el desarrollador? ¿cómo implementa ese flujo? Entonces, hay que tener bien mapeado todo lo posible que pueda llegar a 
ser procesado. Ya sabemos que los datos pueden ser estructurados, semi estructurados y no estructurados. Ahora, siendo 
específicos y llevándola a una realidad concreta, ¿qué puede venir? en datos estructurados podemos estar procesando tablas o 
archivos, porque, podemos tener un archivo CSV que alguien nos deja separado por comas o podemos tener una tabla en Oracle, 
esa es data estructurada. En semiestructurada podemos tener archivos JSON y XML. En esencia, en las empresas básicamente eso 
es lo que entienden por semiestructurado. En un archivo semi estructurado cada registro define su propio juego de campos que 
incluso pueden tener subcampos y no necesariamente los campos y subcampos de todos los registros tienen que ser iguales, pueden 
ser diferentes. En esencia, en las empresas nos podemos encontrar con archivos JSON y XML. 

Ahora respecto a los datos no estructurados, estos ya son los más abundantes. Generalmente los datos estructurados y 
semiestructurados los vamos a tener controlados muy bien desde el principio, porque, en esencia son estos cuatro tipos (Tablas, 
archivos con delimitadores, JSON y XML). De hecho, en los archivos semi estructurados también existe algo llamado BSON, que son 
JSON binarizados, pero realmente las herramientas como Spark que vamos a aprender la próxima semana, son agnósticas al tipo de 
semi estructura, así que nos va a dar igual, pero en el caso de los datos no estructurados el procesamiento de esos datos ya 
depende de la naturaleza de los datos. Para procesar imágenes tendremos que tener una estrategia, para los videos otra 
estrategia, para los audios una estrategia completamente diferente, para documentos enriquecidos como words o pdf otra 
estrategia, así que aquí es donde potencialmente podrían llegar a venir los problemas. Lo importante es que desde el día uno 
en la definición arquitectónica sepamos cuáles son esos formatos no estructurados, tal vez, no resolvamos ese día como vamos a 
capturarlos y procesarlos, pero, al menos hay que tener listados qué tipo de formatos vamos a procesar. 

-----------------------------------------------------------------------------------------------------------------------------

ETL PARA DATOS ESTRUCTURADOS
----------------------------

Entonces a nivel conceptual hay que aterrizarlo a un mayor nivel de detalle para cada tipo de dato que vamos a procesar. Por 
ejemplo, para el procesamiento de datos estructurados, vamos a poner los ejemplos en este momento solamente con procesos batch, 
pero, en el real time es exactamente lo mismo, solo que van a cambiar las tecnologías, así que, por ahora simplemente hablemos 
conceptualmente. En el procesamiento de los datos estructurados para la ingesta, tenemos esta estrategia, habíamos llegado 
hasta el Gateway, exportamos los datos de la Fuente de datos a un Fileserver, los movemos dentro de un Gateway, que puede ser 
un directorio compartido de este Fileserver con el Gateway y subimos el archivo de datos. ¿Donde se sube? a la zona de 
LANDING_TMP. Cualquier cosa que entre al entorno del Data lake, tiene que ir esta zona de LANDING_TMP. Son generalmente 
archivos de texto plano, porque, los están exportando en CSV con algún separador. ¿Dónde van a vivir esos archivos que vamos 
a ir importando? van a vivir en tablas, porque, son archivos estructurados, entonces, si es data estructurada hay que mejor 
ver el archivo como una tabla. Esto tecnológicamente hablando se hace con HIVE. Una vez que hemos ingestado el archivo, ¿por 
qué se llama LANDING_TMP a esta capa? porque su único objetivo es hacer el CTRL + C, capturar el archivo que el día de hoy nos 
están dejando. Ahora en la capa LANDING vamos a mover ese archivo a la partición correspondiente, si estamos capturando la 
tabla ‘Persona’ debe de existir tanto en LANDING_TMP como en LANDING, solo que en LANDING va a estar particionada por fecha y 
en una partición estará el archivo del día de ayer, en otra partición el de que se dio anteayer y el día de hoy se creó una 
nueva partición y ahí se escribe el archivo de datos y de esa manera aquí es donde ya viven los datos acumulados. LANDING_TMP 
solamente es una zona de paso temporal. Adicionalmente a eso al mover el archivo de texto plano y colocarlo en la partición 
correspondiente del día de hoy, nunca hay que colocarlo en texto plano, porque, ya vamos a empezar a procesar esos datos, el 
siguiente paso es, una vez que se capture la data, ahora hay que limpiar los datos, las edades tienen que ser mayores a cero. 
Para hacer procesamiento de datos, los motores de Big data podrían hacerlo en texto plano, pero, esos archivos son 
extremadamente lentos, existen binaria archivos binarios de rápido procesamiento como AVRO, ORC y PARQUET. Acá los vamos a 
binarizar en AVO. La binarización lo único que hace es convertir el archivo de texto plano en un archivo binario que si tu lo 
abres con un editor de texto vas a ver código binario, ya no vas a ver información. Pero si lo abres a nivel de una tabla vas 
a ver datos, si esto tenía 100 registros, pues, vas a ver también los 100 registros ahí. Como son datos estructurados los vamos 
a manejar a nivel de tabla, no a nivel de archivo, así que mejor el que el archivo esté binarizado para que pueda procesarse 
rápido. Adicionalmente a eso, vamos a comprimir también de los datos. La ventaja de hacer una binarización no solamente nos 
ayuda a aumentar la velocidad en las consultas, sino, que también nos ayuda a ahorrar espacio en disco duro. El formato de 
compresión estándar que existe el día de hoy en entornos de Big data se llama SNAPPYPY, que permite paralelizar los códigos, a 
pesar de que la data esté comprimida. SNAPPY es el estándar de facto en compresión para los datos, así que al momento de 
binarizar también vamos a comprimir. ¿En cuánto puede llegar a comprimir SNAPPY? en el mejor de los casos puede comprimir los 
datos hasta en un 80% y recuerda que estamos en un entorno de Big data, si estamos en gestando una tabla de datos de 1 TB, al 
comprimirlo con SNAPPY, ese terabyte que son 1.000 GB de datos, podemos reducirlo a 200 GB de datos y nos estamos ahorrando 
800 GB, eso es bastante información. Por supuesto que la compresión tiene una penalización, las consultas van a correr un 
poquito más lentas de que si solo estuvieran en AVRO, pero miren la ganancia y ahorro de disco duro. El estándar en el caso de 
procesos batch, la data que sea batchera comprímela, pero la data que sea en tiempo real, esa sí no la vamos a comprimir, 
porque, el proceso se va a hacer más lento si lo comprimimos, entonces, ahí no se comprime. Entonces, en la capa LANDING se 
binariza, se comprime y ese archivo binarizado y comprimido se guarda en la partición correspondiente. 

-----------------------------------------------------------------------------------------------------------------------------

FORMATOS SEMI ESTRUCTURADOS
ETL PARA DATOS SEMI ESTRUCTURADOS
---------------------------------

Ahora en el caso de los archivos semiestructurados, la estrategia es la misma. ¿Cuál es la diferencia? los archivos 
probablemente ya no vengan de una base de datos, van a venir de servidores en donde habrá archivos JSON y XML y los tendremos 
que colocar dentro del LANDING_TMP. Pero ya no van a vivir en tablas, porque, son archivos semi estructurados, no los podemos 
ver como tablas. Dentro de LANDING_TMP, digamos que estamos ingestando el archivo ‘Persona’, pues, tendremos que crear un 
directorio ‘Persona’ y dentro colocar ese archivo JSON. Si son archivos semiestructurados, eso se maneja a nivel de directorios, 
debemos de crear un directorio para cada entidad y dentro guardar el archivo. Ahora para moverlo hacia LANDING, ahí vamos a 
repetir el mismo proceso, lo vamos a binarizar y comprimir, esos datos semiestructurados sean JSON o XML pueden ser binarizados 
en AVRO, sólo que ahora la estrategia de particiones se va a manejar el directorio ‘Persona’ y en subdirectorios se irán 
agregando los archivos según fecha. Estos archivos JSON y XML también estarán binarizados en AVRO, ya que, AVRO soporta 
formatos estructurados y también semiestructurados y por supuesto, también tendrán que estar comprimidos. Así es como iremos 
poblando la capa LANDING. 

-----------------------------------------------------------------------------------------------------------------------------

ETL PARA DATOS NO ESTRUCTURADOS
-------------------------------

Ahora, en el caso de los archivos no estructurados, aquí es un poquito diferente. En esencia, la forma en cómo vamos a mover 
los archivos de las fuentes de datos es la misma que el semiestructurado, pero, ya no los vamos a binarizar. La fuente de datos 
tiene archivos de imágenes o videos o audios, da igual el formato de archivo, todo tiene que ir al Gateway. ¿Cómo lo subimos al 
LANDING_TMP? pues es simplemente un directorio, en donde subiremos los archivos de imágenes de ese día, por ejemplo, digamos 
que tenemos grabaciones de una cámara de seguridad y se generaron un video por cada hora de esa cámara de seguridad, entonces, 
tendríamos que subir 24 archivos de vídeo dentro de este directorio y este directorio llevaría el nombre “Grabación de la 
cámara de seguridad 27”, por darle un nombre. Y luego, lo único que tenemos que hacer es, una vez que los datos se ingresaron 
al LANDING_TMP, luego solamente los vamos a mover a la capa LANDING a su partición correspondiente, colocaremos las imágenes 
del día de anteayer, las del día de ayer y las del día de hoy y las del día de mañana. En LANDING ya no vamos a binarizar en 
algún formato de rápido procesamiento como lo hacíamos anteriormente, acá simplemente lo movemos a una partición y de esa 
manera vamos ordenando las capturas de los datos. 

Y listo, con eso ya hemos resuelto el primer problema. Ya sabemos para cualquier nivel de estructura cómo vamos a guardar los 
datos que vamos a capturar dentro del DATA LAKE. Hemos llegado a las capas de LANDING_TMP y LANDING. Así que sea cual sea el 
dato y el nivel de estructura de ese dato, ya sabemos, cómo va a aterrizar en LANDING, lo que es data estructurada y 
semiestructurada se va a binarizar en AVRO y se va a comprimir y lo que es datos no estructurados, bueno pues, simplemente va 
a vivir en las particiones y vamos a ir ordenando lo que estamos ingestando. 

-----------------------------------------------------------------------------------------------------------------------------

CAPA DE MODELAMIENTO
--------------------

Ahora, diariamente ya sabemos que en la capa LANDING los datos se van a ir acumulando y es donde va a vivir toda la 
información. Ahora, ya tenemos la estrategia de captura de datos, vamos a la parte de UNIVERSAL, en donde vamos a hacer el 
modelamiento de los datos. Aquí hay que tener bastante cuidado. ¿Qué significa modelar los datos? cuando hablamos de datos 
estructurados, el modelamiento de datos hace referencia a básicamente a 3 cosas: 

● Seleccionar ciertos campos de la tabla ‘Persona’, que, por ejemplo, vive en la zona de LANDING, castear los tipos de datos 
  correctos. Digamos que la tabla ‘Persona’ tiene el nombre de la persona, su identificación, su salario, su edad, su sexo, su 
  estatura y muchos otros datos. Ahora, hay un rol conocido en las empresas como modelador de datos que define:  “ … mira 
  nosotros somos un banco y a nosotros no nos interesan las estaturas de las personas, nos interesa su nombre, su edad, su 
  salario, su estado civil … “, digamos que estos datos. Entonces el modelamiento de datos implica que de los datos que estamos 
  ingestando no vamos a requerir todos los campos, eso depende de cada negocio, vamos a seleccionar los que realmente va a 
  utilizar negocio. 

● Luego hay que CASTEAR a los tipos de datos correctos, porque por ejemplo, puede ser que originalmente en la fuente de datos 
  que hemos ingestado la edad venga como cadena de caracteres, pero eso tiene que ser un número, hay que darle el tipo de dato 
  correcto, eso es castear, definir el esquema ya del dato, quiénes son número, quién es un entero, quién es un decimal, quién 
  es una cadena de caracteres. 

● Una vez que ya los tenemos como tipos de datos correctos, ahí le aplicamos las reglas de calidad y ahí viene lo que les 
  expliqué, los que cumplan las reglas de calidad se van a una tabla y los que no se van una tabla reyectada. En esencia, a 
  eso nos referimos con modelar, tener los datos listos y dejarlos en la capa UNIVERSAL para que las soluciones los puedan 
  consumir y ya procesarlos. Pero no es tan simple. En el caso de los datos estructurados, lo que les ha explicado es la forma 
  en cómo se trabaja, en el caso de los datos semiestructurados el modelamiento es diferente. DENTRO DE UNIVERSAL LO QUE 
  DEBEMOS DE TENER SON TABLAS ESTRUCTURADAS. ACÁ POR EJEMPLO VAN A VENIR ARCHIVOS SEMIESTRUCTURADOS, DE ALGUNA MANERA QUE AÚN
  NO CONOCEMOS ESOS ARCHIVOS SEMIESTRUCTURADOS LOS VAMOS A CONVERTIR EN TABLAS ESTRUCTURADAS. LO MISMO CON LOS ARCHIVOS NO 
  ESTRUCTURADOS, DE ALGUNA MANERA QUE AÚN NO CONOCEMOS VAMOS A TENER QUE CONVERTIR ESOS ARCHIVOS A FORMATOS ESTRUCTURADOS. De 
  hecho, en algunos casos va a ser imposible colocarlos en tablas, pero sí lo vamos a manejar a nivel de archivos 
  estructurados, así que, EL OBJETIVO DE UNIVERSAL ES TENER LA DATA ESTRUCTURADA, LISTA PARA QUE SEA PROCESADA, ESTRUCTURADA Y 
  CON LAS REGLAS DE CALIDAD. 

-----------------------------------------------------------------------------------------------------------------------------

DEFINICIÓN DEL MODELO DE DATOS [VISTA DE MODELAMIENTO]
------------------------------------------------------

Ahora ¿eso qué significa? bueno va a depender de lo que estemos procesando, vamos a ver cada uno de los casos con los que te 
puedes encontrar en la vida real. Por ejemplo, digamos los datos estructurados, vamos a definir para cada tabla, los campos y 
los tipos de datos que queremos tener. Por ejemplo, de la tabla ‘Persona’ sólo nos interesa estos campos y estos tienen que 
ser los tipos de datos. De la tabla ‘Empresa’ que estamos capturando de alguna fuente de datos, estos campos y estos tipos de 
datos y de la tabla ‘Transacción’ nos interesan estos campos y deben ser de esos tipos de datos. Esto que estoy poniendo de 
una manera muy simple es la vista de modelamiento, es parte de la arquitectura, es definir el modelo de datos. De las tablas 
¿cuáles son los campos y cuáles son los tipos de datos? esto por supuesto no lo hace el arquitecto, recuerden que el objetivo 
de nosotros los arquitectos es balancear todas las vistas para que sean coherentes, esto lo hace otro rol, esto lo hace un 
modelador de datos, el modelador de datos conoce mucho del negocio y él, por ejemplo, si fuera un banco diría: “ … oye el 
campo ‘salario’ nos va a importar a nosotros, porque, en función de eso podemos hacer recomendaciones de aprobar o desaprobar 
créditos, pero la ‘estatura’ de la persona, por ejemplo, eso es irrelevante … “. Ahora, digamos que somos una entidad 
deportiva, entonces, ahí la estatura define ciertas capacidades atléticas, así que, quizá ese campo si le interese a ese 
negocio. La definición de estas tablas de datos la hace un modelador y él nos tendrá que decir de las tablas cuáles son los 
campos y los tipos de datos. Adicionalmente a eso, el equipo de modelamiento, que en la vida real esto es un equipo de varias 
personas, va a depender del tamaño de la empresa, si es una empresa chica puede ser solo una persona, si es una empresa grande 
puede ser un equipo de hasta 20 modeladores trabajando y definiendo. En esencia, ellos solamente definen cómo van a ser las 
tablas, cuáles son los campos y cuáles son los tipos de datos. 

-----------------------------------------------------------------------------------------------------------------------------

REGLAS DE CALIDAD [VISTA DE CALIDAD]
------------------------------------

Paralelamente, se tiene que también definir las reglas de calidad. En la arquitectura eso se le conoce como la VISTA DE 
CALIDAD. Eso no necesariamente lo hacen los modeladores, eso lo puede hacer el equipo de calidad que es un equipo de personas 
completamente diferentes, que conocen ya la naturaleza de las tablas, de las fuentes de datos. Por ejemplo, podrían decir lo 
siguiente: “ … el modelador ¿qué es lo que definió para la  tabla ‘Persona’? estos campos y estos tipos de datos, … “ bien, 
pero a nivel de procesamiento de datos, ¿qué significa datos limpios en esta tabla? por ejemplo, vamos a definir cuatro reglas 
de calidad, los identificadores de los registros no pueden ser nulos, el campo sexo solamente puede tomar 3 valores: masculino, 
femenino y nulo. El salario tiene que ser un número mayor a cero y también el salario tiene que ser menor a 100.000 USD y 
listo esas son las reglas de calidad que el equipo de calidad definió. Misma historia, esto no lo hace el arquitecto, el 
arquitecto solamente se encarga de que haya coherencia entre las definiciones. Esto lo hace el equipo de calidad. 

-----------------------------------------------------------------------------------------------------------------------------

MODELAMIENTO DE DATOS ESTRUCTURADOS
-----------------------------------

La responsabilidad que nosotros tenemos como arquitectos es integrar el modelo definido por los modeladores y las reglas de 
calidad, dentro de la arquitectura y ¿cómo se hace? ahora sí vemos el patrón de diseño, ya sabemos que, lo que hemos ingestado 
desde la fuente de datos (la tabla ‘Persona’ que vive en la zona LANDING y está binarizado en AVRO) en función de lo que el 
modelador haya definido, seleccionaremos de esa tabla algunos campos y lo castearemos a los tipos de datos correctos y en 
función de lo que el equipo de calidad haya definido, en otra tercera cajita debemos de implementar esas reglas de calidad. 
Aquellos registros que cumplan las reglas de calidad aterrizarán en una tabla que se llamará exactamente igual que la otra 
tabla, pero, que vivirá en la zona de UNIVERSAL. Adicionalmente, a eso ¿en las fuentes de datos qué es lo que pasa en la vida
real? por ejemplo, en la fuente de datos el día de hoy, digamos que estamos conectándonos a la tabla ‘Persona’ desde la base 
de datos Oracle y esta tiene 10 campos, pero, por necesidad de negocio en la fuente de datos ahora le agregan un campo 11. La 
ventaja de tener binarizada la información en AVRO, es que nos permiten guardar esa evolución de las estructuras de datos 
dentro de la tabla, cada archivo va a guardar el formato de estructura de tablas con la que fue ingestado el día de hoy, por 
ejemplo, durante 10 años hemos estado ingestando datos que tienen 10 campos y el día de mañana la fuente de datos cambia a 11 
campos, pues, ese nuevo archivo binarizado en AVRO va a incluir dentro del archivo en una zona de metadatos ese nuevo campo 
número 11. Así que esa es la ventaja de AVRO permite tener esquemas flexibles. Pero notemos que en la capa UNIVERSAL ya tenemos 
esquemas bien definidos por los moderadores, entonces, existe un formato binarizado llamado PARQUET, que es más rígido, no es 
tan flexible como AVRO, es ideal para hacer estas implementaciones en donde ya sabemos exactamente qué campos necesitamos. De 
hecho el PARQUET es mucho más rápido que el formato AVRO, porque, ya es un esquema fijo, por eso la capa UNIVERSAL se 
recomienda que esté en PARQUET. En el caso de que los registros no cumplan las reglas de calidad, entonces, esos registros 
deberán de ser enviados a otra tabla que tendrá el mismo nombre de la fuente de datos, pero con un sufijo de “reject” que 
serán reyectados. Adicionalmente a eso, esta tabla ‘Persona reject’ va a tener los mismos campos que la tabla ‘Persona’, pero 
sus tipos de datos van a ser siempre cadenas de caracteres. ¿Por qué? para poder guardar cualquier cosa que se reyecte. Qué 
tal, por ejemplo, si alguien en la edad o en el campo salario se equivocaron y colocaron el nombre de la persona, se reyecta. 
Para poder guardar ese error eso tiene que ser un string, si no, no se podría guardar ese error. Por eso es que esta tabla de 
reyectados tiene que ser siempre de cadena de caracteres. Esto es para el caso de los datos estructurados. 

-----------------------------------------------------------------------------------------------------------------------------

MODELAMIENTO DE DATOS SEMI ESTRUCTURADOS
----------------------------------------

Ahora y ¿qué pasa para los datos semiestructurados? el objetivo con ellos va a ser estructurar los datos. La buena noticia 
aquí es que da igual si son archivos JSON, XML, BSON o cualquier otro tipo de semiestructuras. Las reglas van a ser las mismas. 
Voy a ponerlo de manera muy genérica el día de hoy y ya cuando lo veamos en código vamos a verlo en vivo, primero lo 
entendamoslo conceptualmente. Acá, por ejemplo, cada registro de este archivo JSON tiene datos de Persona, Empresa y datos de 
Transacción. Entonces, podríamos crear 3 tablas la tabla ‘Persona’, ‘Empresa’ y ‘Transacción’. Colocaremos los datos en 
registros de la Persona en una tabla, los de la Empresa en otra tabla y los datos propios de la Transacción en esa tercera 
tabla. Y de esa manera un archivo JSON se convirtió en 3 tablas estructuradas. Una vez que tenemos esas tablas estructuradas, 
lo siguiente es aplicar lo que previamente explicamos: cada una de ellas tiene que seleccionar ciertos campos, castearlos y 
aplicarlas las reglas de calidad y listo, ya tenemos esas tablas estructuradas. También es muy importante entender que estas 
resultantes finales viven en UNIVERSAL, pero, se apoyan en tablas temporales para poder hacer estos procesos de seleccionar, 
castear y aplicar reglas de calidad, así que generalmente hay una zona del DATA LAKE en donde escribimos estas tablas 
temporales que van a vivir solamente para procesar, termina el proceso y como ya tenemos las tablas resultantes finales listas, 
se eliminan para ahorrar espacio en disco duro, eso también es parte del patrón de diseño, solo las resultantes finales tienen 
que ser permanentes, las tablas temporales se eliminan una vez finalizado el proceso. Así que conceptualmente, no hay mucha 
complicación al momento de definir cómo se modelan los datos semiestructurados, el truco es esta parte de “Estructurar los 
datos”. De alguna manera tratar de estructurar los campos en diferentes tablas y luego ya tratarlos como tablas estructuradas. 

-----------------------------------------------------------------------------------------------------------------------------

ESTRUCTURANDO DATOS NO ESTRUCTURADOS [IMÁGENES]
-----------------------------------------------

Ahora vamos a lo que sí va a ser más complejo en la vida real, porque, aquí ya no hay una estrategia genérica como en el caso 
de los datos estructurados o semiestructurados. En el caso de los datos no estructurados va a depender del formato, una cosa 
son imágenes, otra cosa son videos, otra cosa son audios, otra cosa son archivos de texto enriquecido como archivos Word, PDF 
o correos, entonces, ya dependiendo de la naturaleza del dato hay cierta estrategia basada en patrones de diseño. Vamos a 
explicarlo conceptualmente para poder entenderlo. Por ejemplo, en el caso de las imágenes qué es lo que sabemos, nosotros 
sabemos que los colores de una imagen están formados por 3 canales de colores: canal RGB, entonces, de alguna manera desde 
código podríamos separar esta imagen en sus 3 canales de colores, ya sabemos que, la combinación de esos 3 canales de colores 
nos va a resultar la imagen original. Ahora la pregunta es ¿y cómo hacemos esa separación? para eso un archivo de imagen tiene 
cierto tamaño de píxeles, 32 por 32 pixeles, con ciertos canales de colores, generalmente son RGB para poder hacer las 
combinaciones y que salga el color de cada pixel. Lo que podemos hacer es crear 3 matrices una para cada canal de color, cada 
matriz va a tener el tamaño de 32 por 32 casilleros y dentro de cada casillero estará la tonalidad de cada canal de color. Esa 
tonalidad va a ir de cero a 255 y la combinación de las tonalidades nos dará el color de ese pixel. Por ejemplo, digamos que 
este casillero que tiene 11, 70 y 75, entonces tiene más abundante verde y azul y bueno quién sabe qué color dará dentro de la 
imagen. Pero vean cómo estamos ya estructurando aquí los datos, a esto es lo que se le conoce como TENSORIZAR. Dentro de la 
capa UNIVERSAL tienen que vivir las imágenes tenzorizadas, es decir, cada imagen separada en sus canales de colores primarios 
y de qué tamaño es cada una de estas matrices, pues, del tamaño de la imagen original y dentro los casilleros tendrán números 
de 0 a 255, dependiendo de qué tan fuerte sea la tonalidad de ese color. 

-----------------------------------------------------------------------------------------------------------------------------

REPRESENTACIÓN TENSORIAL DE UN DATASET DE IMAGENES
--------------------------------------------------

Ahora esto es justamente lo que se le llama la REPRESENTACIÓN TENSORIAL. Esta es la estructura de datos, digamos que va a 
haber un proyecto que va a ser reconocimiento de imágenes y tenemos 50.000 imágenes, bueno, tendremos que tensorizar cada una 
de esas imágenes en sus canales de colores primarios dentro de estas matrices y de esa manera tenemos datos estructurados.  

-----------------------------------------------------------------------------------------------------------------------------

ESTRUCTURANDO DATOS NO ESTRUCTURADOS [IMÁGENES]
-----------------------------------------------

Entonces el proceso como definición es simple, ya sabemos que en la zona de LANDING vamos a tener en un directorio las 
imágenes, qué es lo que va a vivir en UNIVERSAL, haremos un proceso de tensorización para convertir cada imagen en un TENSOR. 
Cada imagen está tensorizada, representada por sus canales de colores y eso va a vivir dentro de un archivo tensorial dentro de 
algún directorio en UNIVERSAL y de esa manera ya tenemos los datos estructurados para el caso de las imágenes. Ahora, ¿por qué 
es necesario tensorizar? Porque, por ejemplo, las redes neuronales y las redes convolucionales que son soluciones analíticas 
hacen procesamiento tensorial y este va a ser su punto de partida, en la capa de solución ya vivirá la red neuronal o la red 
convolucional. ¿Cuál es el input de las soluciones? lo que hay en UNIVERSAL y en UNIVERSAL ya tenemos tensorizados los datos. 

-----------------------------------------------------------------------------------------------------------------------------

ESTRUCTURANDO DATOS NO ESTRUCTURADOS [VIDEOS]
---------------------------------------------

Y ¿qué es lo que va a pasar con los vídeos? algo parecido nosotros sabemos que un vídeo es un conjunto de fotogramas, 
generalmente cada segundo de un vídeo tiene 24 fotogramas. Lo que debemos de hacer es, bueno, en LANDING va a haber un 
directorio con el vídeo que hemos capturado, tendremos un proceso que separe cada segundo del vídeo en 24 fotogramas, que son 
imagenes y luego ya sabemos cómo procesar las imágenes. Tensorizamos cada una de las imágenes en el, aquí sí es importante el 
orden, tiene que estar tensorizado en el orden, porque, es una secuencia, así que esto es lo que se conoce como un ARCHIVO 
TENSORIAL ORDENADO. Para la PPT anterior el archivo tensorial nos da igual el orden, pero aquí no, en un vídeo la secuencia 
tiene que respetar la secuencia de los fotogramas. Y este tensor ordenado ya vive en un archivo tensorial dentro de la capa 
UNIVERSAL. Esta parte de la separación en fotogramas va a vivir en un directorio temporal, que una vez que ya se tensórice y 
esté todo tensorizado, pues, esto ya se elimina. Así que parte de la arquitectura es tener bien definido eso y esto, por 
ejemplo, también es muy importante, porque, los fotogramas son muchísimas imágenes y ocupan mucho espacio en disco duro y si 
no lo borras pues probablemente se te va a llenar rápido los discos duros del Clúster. 

-----------------------------------------------------------------------------------------------------------------------------

ESTRUCTURANDO DATOS NO ESTRUCTURADOS [AUDIOS]
---------------------------------------------

Ahora y ¿qué pasa con los audios? aquí también hay un patrón de diseño que ya se encarga de hacer el procesamiento. Ya sabemos 
que los audios aterrizan en directorios dentro de LANDING, es simplemente un archivo no estructurado. Ahora lo que tenemos que 
hacer es utilizar algún framework de speech to text que nos convierta la información en un archivo de texto plano de la 
siguiente manera: dentro del audio este framework va a tener que detectar los interlocutores que hay en el audio y qué es lo 
que habla cada interlocutor en cada momento del audio, por ejemplo, el interlocutor 1 del segundo 5 al segundo 15 dijo: “Hola”. 
El interlocutor 2 del segundo 17 al segundo 19 le respondió: “Buenos días” y así sucesivamente. Eso es lo que se conoce como un 
ARCHIVO DE GUIÓN, en donde el audio se transformó a texto, a un guion de texto en donde se sabe qué es lo que dice cada 
interlocutor en cada ventana de tiempo. Eso se hace con algún framework en específico. Una vez que lo tenemos así, lo guardamos 
en una tabla que debe de tener una estructura similar a esta: el ID del interlocutor, la hora inicial y final del mensaje y qué 
es lo que dijo en ese tiempo. Además, estos frameworks en ocasiones agregan más cosas, por ejemplo, una cosa es que alguien 
hable de manera normal y otra cosa es que alguien hable de manera exaltada y eso nos puede ayudar a encontrar un reconocimiento 
de movimiento del sentimiento que hubo en el audio. Por ejemplo, acá puede haber un puntaje, algunos frameworks te lo dan, te 
dicen: “ … la tonalidad de voz está entre el 0 al 10, acá habló con 2 y acá con 2 y aquí se exaltó y llegó hasta 10 … “ eso lo 
podemos guardar también en un campo. Lo importante es entender cómo es que este framework que utilizamos, porque, hay 
muchísimos que podríamos utilizar, cree ese guion y tratar de guardar esa información dentro de una tabla estructurada, para 
luego, ya que alguien consulte esa tabla y empiece a sacar los mensajes en función de las fechas o de lo que necesite. Misma 
historia, este guion solamente es un aspecto temporal, así que, una vez que ya está guardado en la tabla, se elimina y lo que 
vive ya es la información ordenada dentro de la tabla en UNIVERSAL. 

-----------------------------------------------------------------------------------------------------------------------------

ESTRUCTURANDO DATOS NO ESTRUCTURADOS [DOCUMENTOS ENRIQUECIDOS]
--------------------------------------------------------------

Ahora en el caso de los textos enriquecidos como pueden ser los documentos Word, PDF, correos electrónicos o tantas cosas que 
tienen las empresas, aquí lo que tenemos que hacer es, lo que importa no es en sí que, por ejemplo, el documento tenga un 
título que está en arial 24, luego un párrafo que esté en times new roman 23, luego el otro párrafo está en comic sans con una 
tamaño de 40, eso nos da exactamente igual, lo que queremos es la información. Así que el primer paso es extraer la información 
que tenga el documento y convertirla a texto plano, literalmente a un TXT, en donde solamente saquemos lo que está escrito. Por 
ejemplo, si esto fuera un correo electrónico solamente nos interesaría lo que fue escrito dentro de ese correo electrónico. Una 
vez que ya lo tenemos en texto plano, esto lo hace también alguna tecnología, hay muchísimos frameworks, por ejemplo, existe 
una API llamada POI que es de Java que se encarga de hacer traducciones de documentos a texto plano, solamente nos interesa el 
texto. Una vez que lo tengamos en texto plano hay que guardarlo en algún motor de indexación, por ejemplo, un caso de negocio 
que nos podría interesar: digamos que estamos procesando los correos electrónicos y queremos detectar qué mensaje nos envían 
más nuestros clientes, de qué tipo de problemas nos hablan más. Básicamente, lo que tendríamos que hacer es un contador de 
palabras para ver cuál es la palabra que más se repite, quizás si la palabra más repetida es: “se me fue el internet”, sabemos 
que del 100% de los problemas el 90% son relacionados, si somos una empresa de proveedor de internet, 90% es que se les va el 
internet, el 5% es de que está muy lento el internet y el otro 5% son otros problemas. Básicamente, eso lo podemos saber 
haciendo un conteo de palabras, ¿cuáles son las palabras más repetidas? es un poquito más complejo en realidad porque las 
palabras dependen del contexto y bueno solamente lo estoy explicando conceptualmente. Ahora qué herramientas nos permiten 
contar palabras dentro de los textos de manera eficiente y teniendo esos contextos asociados para poder descubrir cuáles son 
los temas que más se repiten dentro de esos contextos. Hay herramientas de indexación de textos, que justamente resuelven este 
tipo de problemas, nos pueden decir cuáles son las palabras o las frases más repetidas dentro de un repositorio textual. Para 
eso lo que se hace es tomar este archivo de texto plano y guardarlo en un JSON que como mínimo tiene 2 partes: el nombre del 
archivo y el texto en plano. Esto va a ir a un motor de de indexación de textos, como lo puede ser ELASTICSEARCH y ahí podremos 
hacerle la consulta. Tenemos acá en elasticsearch un directorio con muchos archivos indexados en JSON, ¿cuál es la frase que 
más se repite? y ya el motor se encarga de buscar esa frase más repetida y encuentra que es “se me va el internet” y listo ya 
sabemos que ese es el tema más problemático que tiene la empresa. ¿Cuál es la segunda frase más repetida? tal y tal y te da 
incluso hasta los porcentajes y ya puedes fácilmente ver qué es lo que más se escribe en los correos electrónicos. Así que 
esos archivos de texto enriquecido se indexan dentro de unos JSON especiales para poder hacer este tipo de búsquedas y hay 
herramientas que hacen ese tipo de búsquedas como lo son ELASTICSEARCH. 

-----------------------------------------------------------------------------------------------------------------------------