{"cells":[{"cell_type":"markdown","metadata":{},"source":["### 1. Instalación e inicialización de Spark"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Actualización de los repositorios de UBUNTU\n","!sudo apt-get update"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Instalación de JAVA\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Descarga de SPARK\n","!wget -q https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Des-zipeado del instalador\n","!tar xf spark-2.4.8-bin-hadoop2.7.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Instalación de Spark en Python\n","!pip install -q findspark"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Configuración de variables de entorno\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Inicialización de Spark\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Verificación de la sesión de Spark\n","spark"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Creación de estructura de directorios para las capas LANDING_TMP y LANDING"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Librería para manipulación del sistema de archivos\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Supongamos que vamos a ingestar del servidor \"SERVER_CLIENTES_EMPRESAS\" dos archivos estructurados\n","#\n","# - ENTIDAD CLIENTE\n","# - ENTIDAD EMPRESA"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creamos el directorio asociado al servidor desde donde vamos a ingestar los datos para la capa \"LANDING_TMP\"\n","os.mkdir('/content/drive/MyDrive/DATALAKE/LANDING_TMP/SERVER_CLIENTES_EMPRESAS')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creamos el directorio para \"CLIENTE\" en \"LANDING_TMP\"\n","os.mkdir('/content/drive/MyDrive/DATALAKE/LANDING_TMP/SERVER_CLIENTES_EMPRESAS/CLIENTE')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creamos el directorio para \"EMPRESA\" en \"LANDING_TMP\"\n","os.mkdir('/content/drive/MyDrive/DATALAKE/LANDING_TMP/SERVER_CLIENTES_EMPRESAS/EMPRESA')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creamos el directorio asociado al servidor desde donde vamos a ingestar los datos para la capa \"LANDING\"\n","os.mkdir('/content/drive/MyDrive/DATALAKE/LANDING/SERVER_CLIENTES_EMPRESAS')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creamos el directorio para \"CLIENTE\" en \"LANDING\"\n","os.mkdir('/content/drive/MyDrive/DATALAKE/LANDING/SERVER_CLIENTES_EMPRESAS/CLIENTE')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creamos el directorio para \"EMPRESA\" en \"LANDING\"\n","os.mkdir('/content/drive/MyDrive/DATALAKE/LANDING/SERVER_CLIENTES_EMPRESAS/EMPRESA')"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Subir archivos a la capa LANDING_TMP"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Subimos los datos de CLIENTE (el archivo 'cliente.txt') a \"LANDING_TMP/SERVER_CLIENTES_EMPRESAS/CLIENTE\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Subimos los datos de EMPRESA (el archivo 'empresa.txt') a \"LANDING_TMP/SERVER_CLIENTES_EMPRESAS/EMPRESA\""]},{"cell_type":"markdown","metadata":{},"source":["### 4. Utilizamos Spark para leer datos de la capa LANDING_TMP como dataframe  "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f8ee68f7-7788-423c-b17f-895fbee5c47b","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+--------------+--------------------------------------+-------------+----+-------+----------+\n","|ID |NOMBRE   |TELEFONO      |CORREO                                |FECHA_INGRESO|EDAD|SALARIO|ID_EMPRESA|\n","+---+---------+--------------+--------------------------------------+-------------+----+-------+----------+\n","|1  |Carl     |1-745-633-9145|arcu.Sed.et@ante.co.uk                |2004-04-23   |32  |20095  |5         |\n","|2  |Priscilla|155-2498      |Donec.egestas.Aliquam@volutpatnunc.edu|2019-02-17   |34  |9298   |2         |\n","|3  |Jocelyn  |1-204-956-8594|amet.diam@lobortis.co.uk              |2002-08-01   |27  |10853  |3         |\n","|4  |Aidan    |1-719-862-9385|euismod.et.commodo@nibhlaciniaorci.edu|2018-11-06   |29  |3387   |10        |\n","|5  |Leandra  |839-8044      |at@pretiumetrutrum.com                |2002-10-10   |41  |22102  |1         |\n","+---+---------+--------------+--------------------------------------+-------------+----+-------+----------+\n","only showing top 5 rows\n","\n","root\n"," |-- ID: string (nullable = true)\n"," |-- NOMBRE: string (nullable = true)\n"," |-- TELEFONO: string (nullable = true)\n"," |-- CORREO: string (nullable = true)\n"," |-- FECHA_INGRESO: string (nullable = true)\n"," |-- EDAD: string (nullable = true)\n"," |-- SALARIO: string (nullable = true)\n"," |-- ID_EMPRESA: string (nullable = true)\n","\n"]}],"source":["# Como son datos estructurados nos conviene verlos como tablas\n","dfCliente = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\"|\").load(\"/content/drive/MyDrive/DATALAKE/LANDING_TMP/SERVER_CLIENTES_EMPRESAS/CLIENTE\")\n","\n","# Mostramos los datos\n","dfCliente.show()\n","\n","# SIEMPRE y SIN EXCEPCIÓN la zona de LANDING_TMP y LANDING tienen que tener todos los campos del tipo STRING\n","dfCliente.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f6989e4e-6ea3-4f92-bbda-3a6f9d2c3d7f","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+\n","|ID |NOMBRE   |\n","+---+---------+\n","|1  |Walmart  |\n","|2  |Microsoft|\n","|3  |Apple    |\n","|4  |Toyota   |\n","|5  |Amazon   |\n","|6  |Google   |\n","|7  |Samsung  |\n","|8  |HP       |\n","|9  |IBM      |\n","|10 |Sony     |\n","+---+---------+\n","\n","root\n"," |-- ID: string (nullable = true)\n"," |-- NOMBRE: string (nullable = true)\n","\n"]}],"source":["# Como son datos estructurados nos conviene verlos como tablas\n","dfEmpresa = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\"|\").load(\"/content/drive/MyDrive/DATALAKE/LANDING_TMP/SERVER_CLIENTES_EMPRESAS/EMPRESA\")\n","\n","# Mostramos los datos\n","dfEmpresa.show()\n","\n","# SIEMPRE y SIN EXCEPCIÓN la zona de LANDING_TMP y LANDING tienen que tener todos los campos del tipo STRING\n","dfEmpresa.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["### 5. Utilizamos Spark para escribir datos de la capa LANDING_TMP en la capa LANDING"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Realizamos la binarización de los datos de \"CLIENTE\" y lo guardamos en el directorio CLIENTE en la capa LANDING\n","# Creamos un subdirectorio de partición para la fecha '2018-04-19' \n","dfCliente.write.format(\"parquet\").save(\"/content/drive/MyDrive/DATALAKE/LANDING/SERVER_CLIENTES_EMPRESAS/CLIENTE/2018-04-19\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Existe algo llamado RETROCESO en los entornos de DATA LAKE. ¿Qué es un retroceso? lo voy a poner en términos muy simples, al día \n","# siguiente viene negocio y dice: “ … oye cometimos un error, el archivo que te pasamos de “clientes” no era el archivo correcto, era otro \n","# archivo, entonces, hay que corregir eso, hay que reprocesar … “. ¿Reprocesar que implica? volver a ejecutar el código y que se reemplace el \n","# archivo binarizado que vamos a escribir en el directorio: /content/drive/MyDrive/DATALAKE/LANDING/SERVER_CLIENTES_EMPRESAS/CLIENTE/2018-04-19.  \n","# ENTONCES, PARA QUE EL CÓDIGO SEA REPROCESABLE SIEMPRE HAY QUE ACTIVAR EL MODO DE SOBREESCRITURA. Así que, si alguien quiere volver a reprocesar \n","# los datos que hemos capturado para cierto día, el código está listo para eso. \n","dfCliente.write.format(\"parquet\").mode(\"overwrite\").save(\"/content/drive/MyDrive/DATALAKE/LANDING/SERVER_CLIENTES_EMPRESAS/CLIENTE/2018-04-19\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Escribimos los datos binarizándolos en la partición de la fecha correspondiente en \"LANDING\" para \"EMPRESA\"\n","dfEmpresa.write.format(\"parquet\").mode(\"overwrite\").save(\"/content/drive/MyDrive/DATALAKE/LANDING/SERVER_CLIENTES_EMPRESAS/EMPRESA/2018-04-19\")"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"ejemplo","notebookOrigID":3340692209454816,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
